{
    "WeaviateShorts": [
        {
            "question": "What is Hybrid Vector Search?",
            "content": "Hybrid search combines vector (semantic) search with traditional keyword search in one query, to return more relevant results. Shortcomings of semantic search for out-of-domain queries can be overcome by combining this with keyword search in a search engine.

            Dense and sparse vectors represent different information so combining them in search can improve relevancy of results. Dense vectors are generated by machine learning (embedding) models like BERT and capture the semantic meaning of data in a numeric representation. Sparse vectors represent the frequency of words in a text document, and algorithms like BM25F compute the relevancy of a document to a query based on primarily the number of keyword matches and their frequency."
        },
        {
            "question": "What is BM25F?",
            "content": "BM25F is a ranking function to rank documents according to their relevance to a search query. It is a keyword-based, TF-IDF-like method, based on the probabilistic relevance model. BM25F is a modification of Okapi BM25, which is applicable to structured documents consisting of multiple fields. BM25F allows weighing of multiple fields in documents, such as headlines, main text, anchor text, according to their field importance. The model preserves term frequency nonlinearity while removing the assumption of independence between same term occurrences.
            
            BM25F can be used together with semantic search in a hybrid approach in a vector search engine."
        },
        {
            "question": "What is the difference between BM25 and BM25F?",
            "content": "BM25F is a modification of the original BM25 ranking function. BM25F adds the ability to weigh individual fields in structured text documents according to their relevancy. BM25F preserves term frequency nonlinearity while removing the assumption of independence between same term occurrences.
            
            BM25F can be used together with semantic search in a hybrid approach in a vector search engine."
        },
        {
            "question": "What is an Approximate Nearest Neighbor (ANN) Algorithm?",
            "content": "An Approximate Nearest Neighbor (ANN) algorithm is a method to find similar vectors in a space which trades off a bit of accuracy (hence the A in the name) for a huge gain in speed.
            
            A nearest neighbor algorithm uses distance-based metrics to identify the closest point to a given point (query) in a space. In case of vector search, the vector search engine aims to retrieve the most similar vectors to a given search query in the vector space. A k-nearest neighbor (kNN) search will retrieve the closest vectors with 100% accuracy, but is computationally very costly. Instead of comparing vectors one by one, vector search engines use Approximate Nearest Neighbor (ANN) algorithms, which trade off a bit of accuracy (hence the A in the name) for a huge gain in speed. Examples of ANN algorithms are HNSW, ANNOY, FAISS, LSH and SCANN."
        },
        {
            "question": "What is HNSW?",
            "content": "HNSW stands for Hierarchical Navigable Small World, a multilayered graph. It is a popular algorithm for approximate k-nearest neighbor search used by vector search engines. HNSW is a proximity graph, in whose lowest layer all data objects are connected based on their proximity (closer objects are linked based on e.g. euclidean distance). On higher levels, fewer data points are connected. When a search query comes in, the closest point in the highest layer will be found, from which the algorithm dives deeper in the layers to find closer datapoints. This approach saves huge amounts of search time compared to kNN methods.
            
            HNSW works based on two techniques: Probability Skip List and Navigable Small World Graphs. A probability skip list is a probabilistic data structure, used to store a sorted list of data elements. It contains layers in which each additional layer of links contains fewer elements. When searching on a higher level, several data points in the entire list are skipped for efficiency. HNSW inherits this format, in which higher levels contain fewer data points for efficient search, and lower levels contain more data points for more accurate search. Navigable Small World (NSWS) graphs are like proximity graphs with both short- and long-range links, in which most nodes are not direct neighbors but can be reached from another node in a small amount of steps. This reduces search time (poly)logarithmically. 
            "
        },
        {
            "question": "What is Weaviate?",
            "content": "Weaviate is a vector search engine that delivers scalable search and recommendation with great DX.
            Weaviate is open source and has a modular design which enables a great variety of use cases. This means you can use Weaviate stand-alone (bring your own vectors) or with out-of-the-box modules to create embeddings and extend the core functionalities with e.g. question answering. For this, Weaviate has integrations with e.g. HuggingFace, OpenAI, Haystack and Jina.
            Built from scratch in Go, Weaviate stores both objects and vectors, allowing for combining vector search with structured filtering and the fault tolerance of a cloud-native database. It is all accessible through GraphQL, REST, and various client-side programming languages.
            Weaviate solves various use cases like (not limited to) Semantic Search, Image Similarity Search, Question-Answering, Recommendation, Hybrid Search, Classification, etc.
            "
        },
        {
            "question": "What is the difference between an ANN Library and a Vector Database?",
            "content": "Vector search databases offer online changeability of data, while with ANN or vector search libraries all the data is indexed once and is not mutable.
            ANN or vector search libraries do approximate nearest neighbor search within a set of pre-indexed data objects. ANN libraries like Annoy, FAISS and ScaNN are built for static use cases, in which you index all the data once and then serve this for searching. Vector databases, on the other hand, can be used for dynamic use cases, where you can add, modify and delete data while serving. This data mutability is the key difference between a library and a database, and can be seen as a key ingredient for production use cases."
        },
        {
            "question": "What is information retrieval?",
            "content": "Information retrieval relates to finding relevant material from a large collection. This may involve finding a relevant piece of text from a corpus or a specific site from the entirety of the Web. The term can refer to the retrieval process or the science related to such processes.

            Broadly, information retrieval requires identifying material that meets an “informational need”. At a practical level, such a need is typically conveyed by a query to an information retrieval system. More concretely, a user entering a search query into the Google search text box or the Weaviate console would both be examples of information retrieval. 
            
            Modern information retrieval systems apply techniques such as building indices to accelerate retrieval. An “inverted index” is one common form where a dictionary of terms catalogs information to search for. A “vector index” is another, more modern form where objects are converted into meaningful numerical representations so as to enable similarity-based retrieval.
            "
        },
        {
            "question": "What is Vamana?",
            "content": "Vamana is a relatively modern, graph-based indexing algorithm built for approximate nearest neighbor (ANN) search applications. Like other ANN algorithms, Vamana builds an index of its constituent points (dataset) in order to allow fast traversal and therefore discovery of k nearest neighbors. 

            Vamanas primary differentiator is its “flat” structure that contains both “short” and “long” edges. It is “flat” in contrast to algorithms such as HNSW that operate using a hierarchical, multi-layer structure with long-range edges in upper layers and short-range edges in lower layers. Instead, Vamana constructs a mix of long-range edges and short-range edges in one layer. This is done by performing multiple passes over the dataset, the first of which introduces short-range edges, and the second of which introduces long-range edges. 
            
            Consequently, Vamana-built indices require fewer reads per query and do not suffer from significant performance degradation when stored on SSDs. Vamana underlies the DiskANN algorithm, which can enable larger vector database systems at a much lower cost than previously thought possible.
            "
        },
        {
            "question": "What is Text Classification?",
            "content": "Text classification is a machine learning task to categorize a piece of text into one of a set of categories. Common examples include classifying a comment as negative or positive, determining whether an email is spam, or categorizing a news item. A model typically outputs probabilities that the piece of text belongs to each of a set of classes. Here the highest probability class is the prediction, and the corresponding probability roughly represents the model’s confidence.

            Text classification has traditionally been seen as a supervised machine learning problem, where a model might be trained and evaluated using a labeled dataset representing the “ground truth”. Such a model would then infer for any new input text the best-suited class. Interestingly, a recent development called “zero-shot” classification has changed this somewhat, in which a general language model is to classify an input using any classes that are newly provided at inference time. 
            
            Notably, a vector database can perform text classification using the kNN classification algorithm, inferring the query objects class based on k nearest neighbor objects. This has benefits of speed and minimal compute requirements. "
        },
        {
            "question": "What is an LSM store?",
            "content": "An LSM (log structured-merge) tree is a data structure that is particularly suitable for applications with high write volumes. An LSM tree store achieves high write speeds as it writes new requests in memory to a sorted, tree-based data structure which is called a “memtable”. The memtable is regularly written to disk, for example when a threshold size is met. This keeps memory demands low, and the written data is “merged” periodically to reduce duplication and segmentation. This produces a multi-level structure where the top level of memtable objects is connected to the lower, disk-based level data.

            To ensure that any in-memory (i.e. memtable) data is not lost upon failure, databases such as Weaviate utilize the “write-ahead-log” (WAL) with an LSM tree. A WAL is an append-only data structure that stores details of updates that are about to be made to the data. As a result, if a crash occurs before a memtable is written to disk, data from the WAL can be used to reproduce the memtable as originally instructed. Thus LSM stores provide a high-speed, robust data structure for use in databases.
            "
        },
        {
            "question": "How to build a vector database?",
            "content": "A database is an organized collection of data, designed to allow the storage and retrieval of information. In this respect, a vector database is no different from a traditional (scalar) one. 

            The main difference comes from the way that the constituent data is indexed. A vector database enables the storage and retrieval of data based on objects’ similarity. This is done by comparing objects’ vector representations. Much as traditional databases utilize data models (e.g. inverted indexes) to improve performance, vector databases do the same by building vector indexes. Modern vector databases make use of ANN algorithms so that they are sufficiently performant. (Note: a vector database is distinct from an ANN library).
            
            Some software libraries also incorporate ways to generate vectors from a data object at import time. Weaviate, for instance, utilizes optional modules such that the user can choose a preferred vectorizer provider or algorithm. The generated vectors are then used to populate the database.
            "
        },
        {
            "question": "Why does a sentence transformer work for semantic search?",
            "content": "A transformer is a type of natural language processing (NLP) model that can be used for converting a piece of text into a vector representation. Transformers differ from simple, word-based vectorizers in that they can better reflect how words affect each other’s meaning, and the meaning of a piece of text as a whole. This is called the “attention” mechanism. 

            Unfortunately, transformer models scale poorly for applications that require similarity comparisons. This is because accurate comparisons with a transformer model require a “cross-encoder” architecture where both sentences are used as inputs. In turn, finding the best matches in a dataset requires calculating similarity values for each sentence pair through the transformer model. This is very computationally expensive and not currently viable for datasets of any significant size. 
            
            Sentence transformers are trained with the goal of eliminating this limitation. Comparisons of sentence transformer output vectors correlate well with their semantic similarities, even though they were each produced using the single source text, rather than a pair of texts. This dramatically reduces the compute requirements for semantic search applications over “vanilla” transformer models, and is the reason for their suitability for semantic search applications.
            "
        },
        {
            "question": "What is FastText?",
            "content": "FastText is a lightweight library for converting text into vector representations. FastText is capable of producing vectors for words or n-grams (multiple, or N, words put together), as well as performing text classification tasks using these vectors. FastText may refer to the set of pre-trained vectors produced with the library, which is available for download and use. 

            A key limitation of FastText in contrast to more modern, transformer-based models is that FastText-produced vectors are not contextually adjusted. That is, FastText will not disambiguate words based on their contexts, whereas transformer models do. On the other hand, benefits of FastText include being able to be run capably without GPU acceleration, and its availability in many languages (157 as of December 2022). Using FastText to produce vectors to represent longer forms of text typically involves applying a weighting scheme (e.g. TF-IDF or BM25) to take a weighted mean of the constituent vectors.
            "
        },
        {
            "question": "What is GloVe?",
            "content": "GloVe (Global Vectors) is a word vectorizer which means that it takes in words and converts them into numerical vectors. This is useful since in this vector space similar words are closer together and dis-similar words are further apart. 

            In forming this word vector representation, GloVe leverages both the global and local co-occurence of words. This idea of incorporating the global co-occurrence of words is what differentiates it from Word2Vec, which is mainly concerned with understanding a word with respect to its neighboring words only. The main intuition behind GloVe is that you can understand the semantic relationships between words by looking at how likely they are to co-occur. It is quite a popular word vectorizer that is readily available and trained on multiple large corpora.
            "
        },
        {
            "question": "What are sparse vectors?",
            "content": "A sparse vector consists of mostly zeros with very few non-zero elements. These are widely found in many fields such as Natural Language Processing where word vectorizers such as Bag of Words (BoW) counts how many times every unique word in your vocabulary is found in a sentence. Since no one sentence will likely ever have all the words in your vocabulary, the majority of the BoW vector will be zeroes or in other words we can say that the BoW vector is a sparse vector.

            Since sparse vectors mainly only consist of zeroes, you can very efficiently compress them so that they don't take up too much memory by only storing the non-zero elements along with their index; every other element need not be stored and is assumed to have a value of zero.   
            "
        },
        {
            "question": "What are dense vectors?",
            "content": "A dense vector consists mostly of non-zero elements. These are very commonly used to efficiently represent different data modalities from images all the way to natural language. Machine learning models are trained to convert data into dense vectors that capture the meaning behind the data. Each element of a dense vector can be thought of as capturing some crucial aspect of the data that is being encoded and represented. As a result each element is quite important and very rarely zero.

            We often take data encode it into dense vector representations so that our machine learning algorithms can efficiently learn from this data.
            "
        },
        {
            "question": "What is anomaly detection?",
            "content": "An anomaly is a very rare observation, datapoint or occurrence that is significantly different from other datapoints, stands out from the pattern that other observations follow, or defies a common rule. 

            For example if you went outside and saw the sky was a bright green color, that would be an anomalous observation. Anomalies don’t themselves follow a pattern so each anomaly can be uniquely different from the common pattern and other anomalous data points. For this reason anomalous data points can be very hard to detect since you can never say what an anomalous data point would look like - they follow no recognizable pattern. A green sky is anomalous but so is a bright purple or pink sky, so you can’t just say that if you want to detect an anomalous sky color, look for bright green skies - all anomalous sky colors are uniquely anomalous.
            
            So the process of anomaly detection requires you to understand what the normal pattern is and detect when a datapoint breaks away from that normal pattern. Anomaly detection is more about detecting when something is not normal rather than trying to detect when something is abnormal. So if you were trying to detect anomalous skies you would be better off saying that a sky that is not blue is anomalous rather than saying a bright green sky is anomalous even though both statements are true."
        },
        {
            "question": "How to use Weaviate on Google Colab?",
            "content": "Weaviate has a Python client, which allows you to connect and interact with your Weaviate instance. This Python package can be used on Google Colab for an easy integration with Weaviate. Please note - you will first need to create an instance on the Weaviate Cloud Service or by generating a `docker-compose.yaml` file.

            The first step is to install the latest version of the Python client. Then, you will connect to your cluster and make sure it is ready to go. Once you’ve uploaded your data and created the schema, you can now query your data right in the Colab notebook. You can use Python, GraphQL, or other languages to run queries.
            "
        },
        {
            "question": "How to use the Weaviate Cloud Service?",
            "content": "The Weaviate Cloud Service (WCS) is the SaaS version of Weaviate. You will first need to sign in or create an account if you haven’t already. If you’re running Weaviate on a local host, you can connect to it and use the console this way. Another option is to create your cluster in WCS. Weaviate offers a free sandbox which is active for 30 days and will be managed for you. 

            Once you’ve connected to the cluster, you can create the schema and populate it with data. You can test the connection using curl or client libraries. In the console, you can use Weaviate’s query language (GraphQL) for data retrieval and mutation. The best part - the console has an autocomplete feature that will assist you with your queries! "
        },
        {
            "question": "What is semantic search?",
            "content": "Semantic search aims to understand the natural language of the query and provide relevant results. It focuses on the meaning of the search query, rather than just the exact words used. This method is advantageous because the exact contents of the query does not need to be stored in the database.   

            If you were to ask, \"what is a Goldendoodle dog?\" the semantic search engine would understand that you're searching for a description of Goldendoodles. If you asked the same question to a traditional keyword search engine, it would suggest articles that mention Goldendoodle, but it might not be relevant to your query. 
            "
        },
        {
            "question": "When should I use Vector Search?",
            "content": "Vector search is a method used for searching through documents or items in a collection based on their semantic meaning. Vector search is an alternative to traditional keyword search. If a query is too broad or the exact words in the query are not in the collection, traditional keyword systems won't perform well. 

            Vector search engines solve this problem by using embeddings that are generated from Machine Learning models. These embeddings can represent various data objects (text, image, audio, etc.) while keeping the context of the data. The exact phrasing of the query does not have to be contained in the database, since vector search is based on semantic meaning. Vector databases and vector libraries use the Approximate Nearest Neighbor (ANN) algorithm to search through vector embeddings. The ANN algorithm offers fast and efficient search, with a slight tradeoff of accuracy."
        },
        {
            "question": "What is Out-of-Domain Vector Search?",
            "content": "Out-of-domain vector search refers to using information retrieval on a dataset that is outside of a particular field or area (i.e. the domain) that the vectorizer was trained on. This distinction exists because a machine learning model’s performance is often inferior when performing the same task on out-of-domain data in comparison to in-domain data. Vector search engines can suffer from the same limitation as the vector embeddings are generated from machine learning models. 

            For example, a vector search engine trained on healthcare data may perform very well at retrieving healthcare documents relevant to a healthcare query. However, the same search engine may fail at retrieving financial information for your finance query. Meanwhile, finding or creating the perfect dataset for training can be extremely difficult. The consequence is that out-of-domain performance is an important metric for many machine learning models, including vectorizers or vector search applications.
            "
        },
        {
            "question": "What is GraphQL?",
            "content": "GraphQL is a data query and manipulation language for APIs, used to build client applications that access data from servers. It was developed and open-sourced by Facebook in 2015, and has since become a popular choice for building APIs in web and mobile applications.

            GraphQL is designed to provide a more flexible and powerful alternative to traditional REST APIs. It allows clients to specify exactly the data they need from the server, which will return only that data, in JSON format, in one call. Clients no longer need to make multiple API calls to fetch different data, which reduces network traffic and improves performance, at the cost of minimal additional complexity.
            
            GraphQL also includes a type system that defines the data available on the server and the operations that can be performed on that data. This allows for stronger type checking and better documentation of the API, making it easier for developers to use and extend.
            
            Here is an example GraphQL query and its result. Learn more at graphql.org."
        },
        {
            "question": "What is deduplication?",
            "content": "Deduplication refers to the process of removing duplicate objects from a dataset. Common applications for end users include removing duplicate items from a catalog, merging different entries for the same contact in a CRM system, or finding very similar images in a collection. In machine learning, deduplication is often performed as part of the data preprocessing step, to reduce the size of training datasets and increase the accuracy of machine learning models by reducing the noise in the data.

            Deduplication can be done based on a set of predefined criteria (e.g. customer records with the same phone number), but this method may not be able to detect more subtle variations or inconsistencies in the data. Another approach, used for deduplicating text records, is Semantic Textual Similarity. It involves measuring the degree to which two pieces of text have similar meanings, rather than just syntax or word choice. This can be done by creating vector embeddings for the two texts then calculating the distance between the vectors, or by using machine learning models trained to predict the semantic relatedness between two pieces of text. You can learn more about these techniques from our blog post comparing bi-encoders and cross-encoders."
        },
        {
            "question": "What is re-ranking?",
            "content": "In vector search, re-ranking is the process of improving the ranking of semantic search results by incorporating additional information or constraints not present in the original query. Such information could include:
            The frequency of the terms in the query, and their rarity among the documents in the corpus. The more often a term appears in the query, and the less often it appears in the documents, the more important it’s likely to be (term frequency-inverse document frequency - TF-IDF).
            A more accurate similarity score between the query and the results, computed using cross-encoders.
            Information from previous queries. For example, querying for “apple” should return different results if the previous query was for “earnings calendar” vs. “healthy foods”.
            User feedback, expressed via upvotes/downvotes, ratings, or simply clicks from the user who initiated the query. Feedback from other users can be considered as well.
            Query expansion: adding to the query terms that are semantically related to the original query, in order to mitigate the mismatch between the language used in a query and in a document. For example, a query for “car” could be expanded to include “vehicle”, “auto”, and “sedan”.
            External information, such as the user’s location used to rank geo-spatial results on a map, or the time of the query to constrain business search results to those that are currently opened.
            "
        },
        {
            "question": "What is vector search?",
            "content": "Vector search is an implementation of semantic search, which is search by similarity, such as by the meaning of text, or objects in images. For example, a search for “science fiction” should find more documents related to the concept of “future”, even though documents titled “Data science” contain the word “science” from the query, while those titled “The future of space travel” do not.

            Vector search uses numeric representations assigned to each data object, as well as to the search query. These representations are called vector embeddings, and in practice they are arrays of real numbers, of a fixed length (typically from hundreds to thousands of elements), generated by machine learning models.
            
            How vector embeddings are generated is beyond the scope of this article, but the important aspect is that two entities with similar semantics will have vector embedding representations that are \"close\" to each other, i.e. that have a \"small\" distance between them. The distance can be calculated in multiple ways, one of the simplest being \"The sum of the absolute differences between elements at position i in each vector\" (recall that all vectors have the same fixed length).
            
            Vector search can be applied to any entities that can be represented numerically. Text is the most common, followed by images, then audio (this is how Shazam recognizes songs based on a short (and distorted) audio clip), but also time series data, 3D models, video, molecules etc.
            
            The magic of vector search resides primarily in how the embeddings are generated for each entity and the query, and secondarily in how to efficiently search within sets of billions of pieces of data."
        },
        {
            "question": "What ANN algorithms are there?",
            "content": "There are many ANN algorithms, and the most popular ones are benchmarked at http://ann-benchmarks.com. ANN algorithms can be grouped in three categories, depending on the data structure used to navigate the search space in memory (most often) or on disk: hashes, trees, or graphs.

            Here are some examples:
            Locality-sensitive hashing (LSH): it uses hash functions to map data points (vectors) to buckets, such that similar points are likely to be hashed to the same bucket. This allows for efficient search of nearest neighbors by only considering points in the same bucket as the query point. However, it is not used in production.
            FAISS (Facebook AI Similarity Search) - released in 2017, scales to 1 billion vectors stored in RAM (compressed to a binary representation, e.g. 32 bytes), or even to 1 trillion vectors stored on disk at the expense of query time. It can trade precision for speed, ie. give an incorrect result 10% of the time with a method that's 10x faster or uses 10x less memory. Faiss broke several performance records at the time, and offers a GPU implementation that is 5-20x faster than on CPUs.
            ScaNN (Scalable Nearest Neighbors) - released in 2020, achieved 2x the performance of state-of-the-art methods at a time, by compressing the dataset vectors to enable fast approximate distance computations.
            Annoy (Approximate Nearest Neighbors Oh Yeah) - developed at Spotify to power music recommendations, using a tree-based indexing approach that minimizes memory and supports many distance metrics. The index is read-only, but once saved to disk as a static file, it can be shared with other processes, which can load (memory map) the file and do lookups immediately.
            HNSW (Hierarchical Navigable Small World) - this algorithm uses a multi-layered graph to efficiently navigate the vector space. The lowest layer captures all objects in the database, then each layer on top has exponentially fewer points that match with the lower layers. Nearest neighbors are found by starting at the closest points in the highest layer, then going from those points one layer deeper, until the lowest layer is reached. The algorithm trades off a slower building process for super fast query times, and is the first ANN algorithm implemented in Weaviate.
            Vamana - graph-based algorithm behind the DiskANN solution. It is similar conceptually to HSNW but using a flat graph, which enables faster access to nodes in the representation of the graph stored on disk.
            "
        },
        {
            "question": "What is NDCG?",
            "content": "NDCG is an evaluation metric for search systems that is particularly useful when there are more than one positive document per query. It is further useful when there are more fine-grained relevance labels than binary, 1 or 0, relevance. For example, the query “Does taking Zinc help recovery from the common cold?” may have multiple relevant documents we want our search system to return. Additionally, these documents may have been labeled to denote that some relevant documents are more relevant than others.

            So now that we know why NDCG is useful, let’s dive into how it’s calculated. Firstly, NDCG is short for “Normalized Discounted Cumulative Gain”. We begin by calculating just the DCG from the ranking outputted by our search system, and then normalize the score with the ideal ranking. The equation to calculate DCG is shown below, pretty much we just loop through the ranked list of p items, rewarding items that have a relevance label, but penalizing them based on how far they are from the beginning of the list. So if a relevant item has been placed at position 100, the DCG will be 1 / log(101), which is a smaller value than if the relevant item is at position 1, with a DCG of 1 / log(2).
            
            After calculating the DCG from the output of our search system, we compare it to the DCG of the human labeled ideal ranking, referred to as the IDCG. This is just a perfectly sorted list such that all the relevant items are at the beginning of the list, leading to something like 1 + ½ + ⅓, … The NDCG is then the DCG score divided by the IDCG score.
            "
        },
        {
            "question": "What does Recall mean in a vector search engine?",
            "content": "Recall, similar to counting Hits, is a metric that reports the number of relevant items that were returned by the system. In search systems, such as a vector search engine, Recall is typically paired with a parameter K denoting the cutoff of ranked items from the search. Recall at K, often shown with an @ symbol for visualization effect, describes whether the correct content was contained in the top-K results. So Recall @ 100 means the correct content was in the top-100, out of potentially billions of ranked results. If we have more than 1 relevant document, Recall @ K returns the fraction of labeled documents contained in the top K, such as 3 found relevant items out of 5 total relevant items. This subtle difference separates Recal @ K distinct from Hits @ K, which does not normalize by the total number of relevant items."
        },
        {
            "question": "What is Average Precision in Search?",
            "content": "Precision is a metric that tells us how many retrieved items are relevant. Precision is typically paired with a parameter K denoting the cutoff of ranked items from the search. Precision at K, often shown with an @ symbol for visualization effect, describes how many of the returned K items have been labeled as relevant for the query. So if we return 10 results and 4 of them are relevant, the Precision value is 4 / 10 = 0.40. This is distinct from recall that would look for how many of the relevant items have been returned. So if there were 8 relevant results and we return 10, the Recall @ 10 for this example would be 4 / 8 = 0.5."
        },
        {
            "question": "What are Evaluation Tactics in Search?",
            "content": "Evaluation tactics allow us to compare the performance of search technologies. Performance is typically measured by accuracy, as well as metrics that are extremely important to real-world usage such as speed, storage, cost, and scalability. For example, we may be interested in the question of: what is a better search technology? BM25 keyword scoring? Dense Passage Retrieval vector search embeddings? Or a Hybrid combination of the two?

            In order to measure accuracy, we construct Search tasks based on “finding the needle in the haystack”, labeling a query with its matching “gold” document. This can be additionally enhanced with multiple “gold” documents and further supplemented with more fine-grained relevance scores other than binary, 0 or 1, labels. We then put that document into the haystack of other documents and see if the search system returns the “gold” document when the original query is given as input. This broadly describes measuring Recall, or Hits @ K, the number of “gold” documents contained in the top K results. We may also look at Precision, the number of returned documents that have been labeled as “gold”. Additionally, NDCG is a more precise measure 
            
            We also want the system to be fast! Speed is typically reported by measuring how long a single query takes, or throughput measures such as Queries Per Second (QPS). Here is an article that describes how Approximate Nearest Neighbor search enables extremely fast vector search!
            
            Cost is another important factor to add in evaluating search systems. For example, we may be able to get a really fast search using brute force on a 4 GPU machine, but then we have to pay for the 4 GPUs… Different Approximate Nearest Neighbor algorithms trade-off speed for storage and machine costs. To learn more, check out this article that describes the difference between Vamana and HNSW search algorithms! Similar to storage, cost is also heavily impacted by how much data we are searching through. The scalability of the system is a unique challenge solved by software like Weaviate. Weaviate’s Cloud Service offers users the ability to completely forget about this. But if interested in self-managing this, Weaviate has docker containers and kubernetes helm charts to facilitate scalability. If curious about more particular details of Distributed Databases and features such as Replication, please check out our Weaviate v1.17 release podcast in which Parker Duckworth and Etienne Dilocker explain the Replication API in Weaviate and the key ideas that make this feature work!
            "
        },
        {
            "question": "What are Graph Embeddings?",
            "content": "Graph embeddings are one of the most exciting emerging ideas in Artificial Intelligence. Similarly to text or image embeddings, this describes representing graphs with vector representations that capture the semantic similarity such that we can search for similar graphs. So, what are graphs?

            Graphs describe a relationship of nodes to other nodes by connecting them with edges. There are many types of graphs such as allowing for multiple types of nodes and edges, and different connectivity patterns. Bringing graphs to the real world, there are tons of objects best represented as graphs such as neighborhoods, molecules, or knowledge bases. Knowledge graphs are a very common strategy to represent facts as nodes with edges to their attributes or relationships with other entities. To conclude, there are many embedding strategies for converting these graphs to vector embeddings such as DeepWalk, RotatE, or Graph Neural Networks!
        "
        }
    ]
}